\chapter{Problemanalyse}

Continuous-Integration und Feature-Branches sind beides Methodiken um die Kollaboration von Entwicklern zu koordinieren. Beide sind aus der fortschreitenden Entwicklung in der Informatik hervorgegangen. Haben neue Techniken und Methoden aufgegriffen und sind zu Teilen repräsentativ für die Zeit in der sie entstanden sind. Während Continuous-Integration nach Stabilität und Kontinuität strebt, entstand die Feature-Branch-Methodik in einer Zeit, in der Flexibilität immer wichtiger wurde.

Die Kollaboration von Entwicklern führt zu nicht trivialen Situation. Zum einen erfordert es Interaktion und Kommunikation. Zum anderen ermöglicht es andere Sichtweisen und Problemlösungen. Ein einzelner Entwickler bleibt zumeist in dem von ihm selbst gesteckten Rahmen.

In professioneller Softwareentwicklung sollte stets das Ergebnis im Fokus stehen. Ergebnisorientierte Softwareentwicklung beginnt bei der Anforderungsaufnahme. Die Anforderungsaufnahme ist meist aufgrund der vielen Daten und daraus resultierenden Interpretationen eine Herausforderung für sich. Die Anforderungen wiederum müssen die Basis für die weitere Softwareentwicklung darstellen und über den vollen Prozess der Erstellung von Software, als notwendiges Erfolgskriterium herangezogen werden.

Die letzten 30 Jahre der Softwareentwicklung mussten sich mit dieser Herausforderung auseinander setzen. Nicht immer ist dies erfolgreich gewesen und zahlreiche Projekte sind daran gescheitert.

TODO: Quellen für Projektscheitern

Über die Zeit haben klassische Bereiche der Informatik, in praxistauglicher Form, Einzug in die kommerzielle Softwareentwicklung gehalten. Software-Validierung und Komplexitätsanalysen, sowie formale Sprachen werden genutzt, um Software gewinnbringend zu entwickeln. Software in moderner Entwicklung, wird semantisch und syntaktisch analysiert um Probleme und Fehler zu finden. Komplexitätsanalysen helfen, schlecht skalierbare Softwarebestandteile zu identifizieren und formale Sprachen werden unter anderem genutzt, um domainspezifische Bereiche getrennt von Entwicklungsdetails zu formulieren.

Trotz der fortschrittlichen Entwicklungsmöglichkeiten, scheitern viele Softwareentwicklungen noch daran diese anzuwenden.

Der Konflikt von Continuous-Integration und Feature-Branches verdeutlicht Teile des Problems. Über den Verlauf der nachfolgenden Kapitel wird schrittweise beleuchtet, welche Bestandteile beide Methodiken ausmacht. Worin der Konflikt beider Methodiken besteht und welche Lösungsansätze dafür existierten. In Kapitel~\ref{ch:visu_meth} werden weiterführende Methoden besprochen und entwickelt.

Als Einstieg werden die bekannteren Vorgehensmodelle der letzten 30 Jahre beleuchtet und auf die sich verändernde Entwicklungskultur eingegangen.

\section{Vorgehensmodelle}

Um komplexe Software in einer strukturierten und definierten Herangehensweise zu erstellen, wurden ausführlich
beschriebene und wohl definierte Vorgehensmodelle erarbeitet. Deren Ziel ist es eine klare Schrittfolge mit 
eindeutigen Zielstellungen vorzugeben. Diese Schritte gliedern den komplizierten und komplexen Entwicklungsprozess 
in klar abgegrenzte und bewertbare Abschnitte. 

Die bekanntesten Vorgehensmodelle halten sich an eine klare Folge aus Planungs-,\\ Konzeptions-, Implementierungs-, 
Test- und Abnahmephase. Durch die klare Trennung der Phasen werden diese Modelle auch statische Vorgehensmodelle genannt.
Beispiele sind hier das Wasserfallmodell\footcite[vgl.][Vol. 11]{royce-1987} und das V-Modell. Insbesondere in
Bereichen mit hohen Sicherheitsanforderungen und einer starken Bindung an bürokratische Strukturen, wird auch heute noch
das V-Modell XT erfolgreich eingesetzt\footcite[vgl.][]{v-model-xt-bund}.

\begin{figure}[htbp]
  \includegraphics[
    width=\textwidth,
    height=\textheight,
    keepaspectratio
  ]{resources/vorgehens-modelle-zeitachse.pdf}
  \caption{Vorgehensmodell zeitlich sortiert}
  \label{process-models-time-line}
\end{figure}

Mit der Komplexität einer Softwareanwendung steigt ihr Umsetzungsaufwand. Dieser zusätzliche Aufwand teilt sich in statischen Vorgehensmodellen in die einzelnen Phasen auf. Bei besonders umfangreichen Softwareprojekten entsteht daher eine signifikante zeitliche Diskrepanz zwischen den Phasen der Anforderungsaufnahme und deren Umsetzung. Des Weiteren ist der Vorgang der Anforderungsaufnahme schwierig und kann nur schwer auf Vollständigkeit geprüft werden. Diese beiden Faktoren erhöhen das Risiko eine fehlerhafte Softwareanwendung zu erstellen. Aus der hohen Dauer der einzelnen Phasen ergibt sich zudem eine hohe Reaktionszeit um fehlerhaft umgesetzte Anforderungen zu korrigieren.

Ein Projekt zur Umsetzung einer komplexen Softwareanwendung benötigt ein umfangreiches Risikomanagement, um Folgekosten 
zu verringern.

Um den Aufwand für das Risikomanagement und die Folgekosten zu minimieren entwickelte sich eine alternative
Herangehensweise, welche vor allem Interaktion, Funktionalität, Kundenorientierung und Veränderungsbereitschaft als
wichtig erachtet. Festgehalten im Agilen Manifest\footcite[vgl.][]{agile-manifest} proklamierten viele bekannte und angesehene
Softwareexperten ihren Willen, Softwareprojekte grundsätzlich anders zu fokussieren.

Beeinflusst von der Lean-Bewegung aus der Automobil-Fertigungsindustrie\footcite[vgl.][]{kent1999} wurden die abgegrenzten Phasen 
in den statischen Vorgehensmodellen in Frage gestellt. Hohe Eigenverantwortung und kurze Kommunikationszyklen sind die 
Basis des neuen Vorgehens.

Die Prinzipien des \glqq Agilen Manifestes\grqq{} stellen hohe Ansprüche an Entwickler und erzwingen ein Umdenken im Umgang mit dem Projektprozess. Während in den statischen Vorgehensmodellen jede Phase eine längere zeitliche Periode einnimmt, so werden diese Phasen in agilen Vorgehen deutlich verkürzt und verschwimmen teilweise. Dadurch werden Kommunikationszyklen verkürzt und Informationsflüsse deutlich beschleunigt.
Durch die schnellere Zyklen sinkt die benötigte Zeit zur Reaktion auf Probleme und Hindernisse. Damit ist es möglich, kontinuierlich und zeitnah Verbesserungen für den Entwicklungsprozess und die Softwareanwendung einzubringen. 

Agile Vorgehensmodelle und Techniken aus der Lean-Bewegungen setzen stark auf eine Arbeitseinteilung anhand von Produktmerkmalen(Features)[quote Scrum, quote Kanban]. Der von der agilen Bewegung angestrebte starke Produktfokus und die kurzen Iterationen vereinfachen die Bewertung des Projektprozesses und der verwendeten Hilfsmittel.

Der hohe Fokus auf Produktmerkmale und das Prinzip zur frühen und kontinuierlichen Auslieferung von wertvoller Software\footcite[vgl.][]{agile-manifest-principles} helfen die Akzeptanz für die Software zu steigern. Außerdem können falsch interpretierte Anforderungen früher erkannt werden. 

Um die Umsetzung der Anforderungen zu prüfen, wird eine Qualitätssicherung benötigt. Für komplexe Softwareanforderungen ist die Prüfung der Anforderungen mit sehr hohem Aufwand verbunden, wenn sie manuell ausgeführt wird. Um Anforderungen automatisiert zu prüfen, müssen diese abstrakt formuliert werden. Der damit verbundene Aufwand steigt, wenn sich die Anforderungen häufig ändern. Dies sollte in einer Strategie für die Qualitätssicherung beachtet werden. 

\section{Qualitätssicherung und Softwaretest}

Die Erfüllung der Anforderungen an eine Softwareanwendung sollte durch eine Strategie der Qualitätssicherung sichergestellt sein. Da jede Änderung an der Software die Erfüllung der Anforderungen beeinträchtigen könnte, sollten die Anforderungen nach Änderungen erneut geprüft werden. Da manuelle Prüfungen aufwändig und zeitintensiv sind, sollten sie sich auf wenige Anwendungsszenarien beschränken. Diese Anwendungsszenarien sollten nur schwer oder unzuverlässig durch programmierte Prüfungen abgedeckt werden können.

Programmierte Prüfungen, auch Tests genannt, zu erstellen kann sehr komplex und kompliziert sein. Je nach Softwareanwendung und damit verbundenen Anforderungen, müssen Kompromisse zur Laufzeit, Aussagekraft und Wartbarkeit eines Tests eingegangen werden.

\subsection{Funktionale Tests}

Abhängig von den Vor- und Nachteilen der verschiedenen Testarten, werden Tests auf verschiedenen Abstraktionsebenen der Software ausgeführt. Es wird zunächst zwischen funktionalen und nicht-funktionalen Testkriterien unterteilt.
Bei den funktionalen Tests wird weiter zwischen Akzeptanz-, Integrations- und Unittests unterschieden\footcite[S.159][]{software-quality2008}. 

\paragraph{Akzeptanztest}
oder auch Abnahmetests stellen in erster Linie die Erfüllung der definierten Anforderungen sicher. Akzeptanztests werden häufig in der Form von Anwendungsszenarien (Use-Cases) beschrieben. Anwendungsszenarien beschreiben das Zusammenspiel von Systemen, deren Akteuren und Aktionen die zwischen diesen ausgeführt werden. Die Verwendung von technischen Details der Software, wie Datenbankspezifikationen oder konkreten Systemimplementierungen, sollten vermieden werden. Nicht nur erhöht es den Wartungsaufwand dieser Tests, es verschiebt auch den Fokus des Tests, weg von den eigentlich zu prüfenden Anforderungen.

\paragraph{Integrationstests}

dienen der Überprüfung von Schnittstellen. Dabei wird das Verhalten der Schnittstelle auf definierte Ein- und Ausgaben geprüft. Der Fokus der Tests liegt darauf unerwartetes Verhalten beim Zusammenspiel mehrere Komponenten auszuschließen.

\paragraph{Unit-Tests}

werden für Objektklassen- und Methodentests verwendet. Das erwartet Verhalten der zu überprüfenden Einheit(Unit) wird mit dem Unit-Test sichergestellt. Zugleich übernimmt er auch eine dokumentarische Funktion. Durch die Initialisierung der Einheit und der Aufruf ihrer Schnittstelle, wird die Intention ihres Autors deutlich.

\subsection{Nicht-Funktionale Tests}

Die nicht-funktionalen Testkriterien betreffen Merkmale, welche nicht einem einzelnen Anwendungsfall zugeordnet werden können. Es werden Leistungsaspekte und unterbewusste Kriterien geprüft. Solche Merkmale sind zum Beispiel Reaktionszeit der Anwendung, wie viele Aktionen pro Zeiteinheit die Anwendung bewältigen kann und wie gut die Anwendung nutzbar ist (Usability). Nicht-funktionale Tests sollten zudem weiter nach der Möglichkeit der Bewertung ihrer Ergebnisse unterteilt werden. Gut zu bewerten sind Leistungs- und Sicherheitstests. Weniger gut zu bewerten sind Explorativ- und Usability-Tests.

\paragraph{Leistungstests} oder auch Performance-Tests werden genutzt, um die Leistungsfähigkeit einer Softwareanwendung zu erproben. Kriterien wie Anfragen pro Zeiteinheit, maximale Anzahl paralleler Anfragen und Reaktionszeit, sowie Anfragedauer werden getestet.

\paragraph{Sicherheitstests} werden genutzt um Schwachstellen einer Softwareanwendung zu finden. Es werden Listen von bekannten Sicherheitsproblemen\footcite[vgl.][]{owasp-vulnerability} genutzt und in der Anwendung geprüft.

\paragraph{Explorativ-Tests} werden manuell durchgeführt und versuchen Fehler zu finden, die auf anderen Wegen nicht oder kaum zu entdecken sind. Explorative Tests sollen, durch kreative und unerwartet Verwendung der Software, bisher unbekannte Schwäche finden.

\paragraph{Usability-Tests} werden genutzt um die Benutzbarkeit der Softwareanwendung zu ermitteln. Während Akzeptanztests ermitteln ob Funktionalitäten verfügbar sind, untersuchen Usability-Tests ob die Funktionalitäten benutzbar sind. Usability-Tests werden meist manuell durchgeführt und sind aufgrund ihrer subjektiven Natur schwer zu bewerten. Es ist daher üblich, auf eine Untergruppe dieser Tests zu prüfen, wie zum Beispiel die Barrierefreiheit einer Anwendung.

\subsection{Testperspektiven}

Die Abbildung~\ref{agile-testing-quadrants} fasst die verschiedenen Tests anhand der Art der Durchführung und anhand ihrer Perspektive auf die Tests, in vier Quadranten zusammen.

\begin{figure}[htbp]
  \includegraphics[
    width=\textwidth,
    height=\textheight,
    keepaspectratio
  ]{resources/agile-testing-matrix.pdf}
  \caption{Agile Testing Matrix\protect\footnotemark}
  \label{agile-testing-quadrants}
\end{figure}
\footcitetext[vgl.][Kapitel: The Agile testing matrix]{cd-docker-jenkins}

\paragraph{Business-Facing} gliedert Tests, welche aus Anwendersicht wichtig sind und den kommerziellen Erfolg der Softwareanwendung sicherstellen. Diese decken zum einen Akzeptanztests ab und stellen die Hauptfunktionalitäten sicher, zum anderen Usability-Tests um die Nutzerfreundlichkeit und Bedienbarkeit zu prüfen.

\paragraph{Support Team} gliedert Tests, welche Regressions-Sicherheit sicherstellen. Da die Anlaufstelle für Fehler im Allgemeinen der Support ist, wurde hier diese Bezeichnung gewählt.

\paragraph{Critique Product} gliedert Tests, welche kritische Bereiche abdecken. Dies umfasst zum einen Randerscheinungen und Grenzfälle, welche nur explorativ entdeckt werden können und zum Anderen Leistungs- und Sicherheitstests.

\paragraph{Technology Facing} gliedert Tests, welche häufig keinem direkt Anwendungsfall zugeordnet werden können. 
Gegenüberliegende Ansichten stellen auch gegenläufige Interessen dar. Daher ist ein Gleichgewicht der Ansichten über den Quadranten erstrebenswert.

\section{Automatisierung der Softwareerstellung}
\label{sec:automation-software}

Zur Bereitstellung einer Softwareanwendung werden mehrere Schritte benötigt. Es wird ein konkreter Stand der Softwarequellen benötigt, ein ausführendes System und es muss für eine Kompilierung der Softwarequellen zu dem System gesorgt werden.
Je nach Anforderung und Art der Softwareanwendung entspricht dieser Vorgang einer trivialen Kopie oder einem mehrstufigen, komplexen Vorgang. 

Im Nachfolgenden wird beschrieben wie Software in einen wohldefinierten, funktionalen Zustand gebracht und dieser validiert wird. Dazu wird auf Versionsverwaltungssysteme, Konfigurationsmanagement, die automatisierte Erstellung der Softwareanwendung und deren Test eingegangen.

\subsection{Codeverwaltung und Versionsverwaltungssysteme}

Programmversionen können bei fast jeder käuflich zu erwerbenden Softwareanwendung unterschieden werden. Potentielle 
Anwender können so bereits vor Erwerb der Software transparent nachvollziehen, um welche Auflage es sich handelt. Anwender können erwartet für ihre Programmversion Fehlerkorrekturen zu erhalten. Um das Fehlverhalten verschiedener Versionen nachvollziehen und beheben zu können, ist es notwendig diese zur Verfügung zu haben. Der jeweilige Quellcode sollte daher in einer entsprechenden Ablage verfügbar sein.

Die einfachste Form der Ablage ist eine manuelle Kopie der Softwareanwendung. Mit der Weiterentwicklung der Softwareanwendung und
dem Entstehen weiterer Programmversionen zeigen sich schnell Schwächen dieses Systems. Wenn in einer frühen 
Programmversion ein Fehler gefunden und behoben wird, sollte dieser auch in allen anderen folgenden Versionen behoben 
werden. Dadurch entsteht mit dieser einfachen Ablage ein erheblicher Mehraufwand. Fortschrittliche 
Versionsverwaltungssysteme bieten Möglichkeiten diesen Mehraufwand zu reduzieren.

Zustandssicherheit ist ein besonders wichtiges Merkmal für Versionverwaltungs- oder auch Versionskontrollsysteme.
Die Reproduzierbarkeit eines definierten Zustandes der Softwarequellen. Der definierte Zustand der Softwareanwendung ergibt sich
aus der Abfolge aller Änderungen, die an der Softwareanwendung vorgenommen wurden, bis zu der jeweiligen Programmversion. 

Als weiteres Merkmal ist die Transparenz aller getätigten Änderungen anzuführen. Die Nachvollziehbarkeit der Änderungen 
hilft oft bei der Koordinierung der Entwicklung mit mehreren Programmierern. Außerdem ist es sehr hilfreich zu wissen, 
welcher andere Projektteilnehmer bei Problemen mit einem Code-Abschnitt gefragt werden kann. 

Über die Entstehung von Versionsverwaltungssysteme[quote Soft.Quali] haben sich vor allem zwei Prinzipien herausgebildet: 
zentrale und dezentrale Versionsverwaltungssysteme. Zwar unterscheiden sich die Versionsverwaltungssysteme auch in 
anderen Merkmalen, zum Beispiel in der Ablage der Quellen und deren Änderungen, diese Merkmale haben aber nahezu keinen 
Einfluss auf die Verwendung im Softwareentwicklungsprozess.

\paragraph{Zentrale Versionsverwaltungssysteme} folgen dem \glqq single point of truth\grqq{} Prinzip. Es existiert nur ein
zentraler Punkt auf dem alle Versionstände gespeichert werden. Dies ermöglicht einen einfachen Kontrollfluss und führt nur zu geringfügigen Abweichungen unter allen beteiligten Entwicklern. Damit einhergehend ist eine Übersicht aller Änderungen problemlos möglich. 
Da alle Daten zentral zur Verfügung stehen, werden deutlich weniger Daten bei den jeweiligen Nutzern der Versionverwaltung notwendig.

Nachteil dieser Variante ist häufig der initiale Aufwand, da eine zentrale Komponente bereitgestellt werden muss. Zudem 
sind zentrale Systeme generell störanfälliger und weniger ausfallsicher. Der Vorteil des \glqq single point of truth\grqq{} führt
auch zum Nachteil des \glqq single point of failure\grqq{}, wodurch Probleme alle Teilnehmer gleichzeitig betreffen.
Je nach verwendeter Versionierungsstrategie, kann es zudem zu einer zentralen Sperrung einzelner Bereiche der Softwarequellen kommen, wenn diese für eine exklusive Bearbeitung gesperrt werden.

\paragraph{Dezentrale Versionsverwaltungssysteme} benötigen im Gegensatz keine zentrale Instanz und können problemlos in einem losen Verbund von Einzelsystemen betrieben werden. 
Häufig wird innerhalb des dezentralen Verbundes eine Instanz als Hauptknoten vereinbart. Da der Abgleich mit dem Hauptknoten nicht für jeden Änderung erfolgen muss, ergibt sich eine hohe Flexibilität für den Entwickler. Änderungen können vom Entwickler nach Belieben verschoben und kombiniert werden.
Häufig werden jedoch Restriktionen hingenommen, die ein kontrolliertes gemeinsamen Arbeiten ermöglichen. Beim gemeinsame Arbeiten ist besonders auf die Veränderung bereits geteilter Inhalte zu achten.
 
Dezentrale Systeme verwalten jeweils eigenständig eine Kopie der geteilten Inhalte. Dadurch ergibt sich eine hohe Redundanz, welche sich positiv auf die Ausfallsicherheit und Verfügbarkeit der Inhalte auswirkt. Im Gegensatz dazu entsteht ein Mehraufwand bei der Koordinierung der Einzelsysteme und in Summe ein höherer Speicheraufwand.

\subsection{Konfigurationsverwaltung}
\label{subsec:konfigurationsverwaltung}

Konfigurationsverwaltung ist eine wichtige Basis für die automatisierte Erstellung eines Softwareprojektes. Sie definiert wie alle Teilstücke, auch Artefakte genannt, in einem Softwareprojekt miteinander interagieren. Im Detail wird dabei Erstellung, Identifikation, Validierung und Ablage geregelt. Die Definition und Konfiguration der Ausführungsumgebung der 
Artefakte ist ebenso ein Teil des Konfigurationsmanagement. \footcite[vgl.][]{humble2010}

Ein lockerer Umgang mit der Konfigurationsverwaltung kann gerade zu Beginn eines Projektes ohne nennenswerte Folgen einhergehen. Wird die Software über einen längeren Zeitraum entwickelt, treten nach und nach mehr Defekte auf. Diese sind unter anderem auf ein mangelndes Konfigurationsmanagement zurück zuführen. Hierbei wird auch von der Alterung des Softwaresystem gesprochen.\footcite[vgl.][]{software-quality2008}. Gerade wenn sich unbemerkt die Version oder Konfiguration der Software eines Drittanbieters ändert, kann dies schwer zu identifizierende Probleme nach sich ziehen. In anderen Fällen reicht es auch, wenn sich die Konfiguration zwischen Entwicklungs- und Produktivumgebung leicht unterscheidet. Konfigurations- und Abhängigkeitsfehler können leicht einen sehr hohen 
Behebungsaufwand verursachen.

Im Idealfall kann eine vollständige Konfigurationsverwaltung:
\begin{itemize}
\item ein durch Betriebssystem und installierte Softwareanwendungen definiertes System bereitstellen, wobei Versionen und Konfiguration alle Elemente Teil der Definition sind.
\item Systeme aktualisieren, sowie Abhängigkeiten und Konfigurationen synchronisieren.
\item alle Änderungen mit Zeitpunkt und Autor übersichtlich darstellen
\item Sicherheitsregeln und Konventionen über alle Systeme sicherstellen
\item alle diese Funktionalitäten barrierefrei dem Entwicklungsteam zur Verfügung stellen
\end{itemize}

Alle Forderungen sind nicht immer notwendig, helfen allerdings erheblich bei der Wartung und Betreuung der einzelnen Systeme. 
Eine Virtualisierung der verwalteten Systeme und eine Versionsverwaltung der verwendeten Konfiguration und Skripte ist ebenfalls
hilfreich. Die Einhaltung der Richtlinien garantiert die gute Skalierung und Wartbarkeit für das Konfigurations- und Systemmanagement. Das Hinzufügen und Administrieren neuer System bleibt somit beherrschbar\footcite[vgl.][]{humble2010}.

\subsection{Abhängigkeitsverwaltung}
\label{subsec:dependency-management}

In der Abhängigkeitsverwaltung werden die Softwareabhängigkeiten der Softwareanwendung beschrieben. In auf Komponenten
basierenden Softwareanwendungen schließt dies die eigenen, internen Abhängigkeiten mit ein. Diese Abhängigkeiten, meist Module
genannt, sollen die mit monolithischen Strukturen einhergehenden Nachteile unterbinden. Monolithen weisen häufig eine 
schwache Softwarearchitektur auf und entwickeln sich daher entsprechend schlecht weiter. Mögliche Skalierungen des 
Systems oder ein Austausch von einzelnen Bestandteilen sind mit einem erheblichen Mehraufwand verbunden.

Die Aufteilung einer Softwareanwendung auf mehrere Komponenten erhöht hingegen die Skalierbarkeit und Austauschbarkeit. Zudem
ergeben sich Vorteile für das verteilte Arbeiten von Teams. 
Eine zu starke Aufteilung der Softwareanwendungsstruktur oder eine Teilung an der falsche Stelle, kann hingegen zu gegenteiligen
Effekten führen. Ein deutlicher Mehraufwand und eine schlechte Skalierbarkeit sind die Folge. Um die verschiedenen Module 
und Pakete zu organisieren empfiehlt sich eine Abhängigkeitsverwaltung. Mit einer Abhängigkeitsverwaltung können Abhängigkeiten durch Identifikationsterme und Versionsnummern eindeutig beschreiben. Dadurch lassen sich Schwierigkeiten unter den Abhängigkeiten leichter erkennen und Aktualisierungen der Versionen entsprechend der Abhängigkeiten vornehmen.

Für die automatisierte Erstellung der Softwareanwendung ist es erforderlich, ähnlich wie bereits bei der 
Versionsverwaltung, ein eindeutiges Abhängigkeitsnetz zu beschreiben. Zum Auflösen der Abhängigkeiten werden meist 
mehrdeutige Ausdrücke verwendet, die eine Bandbreite an Versionen der Abhängigkeiten erlauben. Wird die Softwareanwendung
erstellt und ausgeliefert, sollte allerdings eindeutig sein, welche Abhängigkeiten verwendet werden. Daher werden in 
vielen Packetverwaltungen diese Informationen in einer Lock-Datei festgehalten. Es wird zudem angeraten, diese Datei in 
die Versionkontrolle aufzunehmen.\footcite[vgl.][]{composer-why-lock-in-vcs} Das Festhalten der exakten Versionsnummern der verwendeten Abhängigkeiten ist ein deutlicher Vorteil. Dadurch ist möglich immer wieder zu dem in der Version gesicherten Zustand der Softwareanwendung zurückzukehren. Ohne dies wären die Abhängigkeiten der Anwendung variabel.

\subsection{Automatisierte Erstellung und Auslieferung}

Viele Softwareanwendungen müssen kompiliert werden, um von einem System ausgeführt werden zu können. Die Kompilierung von kleine Programmen ist eine schnelle und wenig aufwändige Aufgabe. Durch das Wachstum einer Softwareanwendung und das Hinzukommen weiterer Abhängigkeiten und Konfigurationen wird die Ausführung deutlich zeitintensiver und es werden mehr Einzelschritte benötigt. Der vollständige Ablauf wird dann im Allgemeine \glqq Build\grqq{} genannt. Die beim Build ausgeführten Ablaufbeschreibungen werden meist als Build-Skripte bezeichnet. Allgemein wird für den gesamten Vorgang auch von Build-Prozess gesprochen.

Build-Skripte werden häufig nicht als Software-Code wahrgenommen und nicht mit den gleichen Qualitätsanforderungen behandelt. Häufig werden sie nicht in ein Versionsverwaltungssystem aufgenommen und Entwicklungsprinzipien, wie Modularität und Lesbarkeit, werden vernachlässigt\footcite[vgl.][Kap. Build and Deployment Scripting]{humble2010}.

Build-Skripte sollten den Prinzipen von Domain-Driven-Design\footcite[vgl.][The Building Blocks of a Model-Driven Design]{evans-domain-driven} und Modularisierung folgen. Deutlich Abgrenzbare Phasen und sollten in eigene Skripte ausgelagert werden\footcite[vgl.][Kap. Principles and Practices of Build and Deployment Scripting]{humble2010}.

Build-Skripte müssen konsistent wiederholbar sein\footcite[vgl.][Sektion: Ensure the Deployment Process Is Idempotent]{humble2010}. Dafür ist es notwendig bei jeder Ausführung einen wohl definierten, bekannten Zustand herzustellen, auf dem die Software erstellt werden kann. Darauf aufbauend muss jede Komponente einer Software erstellt werden und abschließend die Software selbst.

Die erstellten Komponenten und Teilkomponenten werden auch \glqq build artifacts\grqq{} oder Artefakte genannt. Die Build-Artefakte sollten eindeutig gekennzeichnet werden. Die Kennzeichnung muss einen Rückschluss erlauben, welchen Stand des Software-Codes das Artefakt enthält. 
Eindeutig gekennzeichnete Artefakte können gesammelt und zentral bereit gestellt werden. Die Verwendung des Artefaktes beschleunigt den Software-Erstellungsvorgang, unter Berücksichtigung der Eindeutigkeit und Wiederholbarkeit des Ergebnisses. 
Die zentrale Sammlung von eindeutig gekennzeichneten Build-Artefakten, wird als \glqq build repository\grqq{} bezeichnet.

\subsubsection{Risiken von Build-Skripten}

Build-Skripte gewinnen an Komplexität, wenn sie gemeinsam mit der Softwareanwendung wachsen. Die Skripte sollten als Teil der Anwendung verstanden werden und gleichwertigen Qualitätskriterien unterzogen werden. Nur wenig gewartet Build-Skripte weisen meist eine hohe Komplexität auf.

Zu komplexe Build-Skripte, lange Wartezeiten im Build-Prozess und damit lange Rückmeldungszeiten, führen zu einer geringeren Commit-Frequenz und damit einhergehend zu komplizierten Zusammenführungen von Änderungen. Durch das Zusammenspiel der verschiedenen Probleme, wurde das zurückhaltende Interagieren mit dem Versionsverwaltungssystem und dem Continuous-Integration-Prozess auch als \glqq integration angst\grqq{} bezeichnet. Insgesamt wird das Voranschreiten der Softwareanwendung dadurch deutlich behindert\footcite[vgl.][S. 3 ff]{ci-is-not-build-systems}.

Werden bei einem Build-Skript Aspekte der vorangegangen Abschnitte zu Versions-, Konfigurations- und Abhängigkeitsverwaltung nicht berücksichtigt, können zudem inkonsistente und damit nicht
wiederholbare Resultate erzeugt werden. Diese können zu schwer oder nicht auffindbaren Fehlern führen.

\subsubsection{Automatisierte Auslieferung}

Die Auslieferung der Software ist direkt mit der Erstellung verwoben. Die Verfügbarkeit aller Abhängigkeiten auf dem Zielsystem und die richtige Konfiguration aller Softwarebestandteile ist notwendig für die Lauffähigkeit der Software.
Ein vollständiges Konfigurationsmanagement beschreibt die Systemabhängigkeiten und sorgt folglich für die korrekte Zulieferung der beschriebenen Artefakte.

Über den Erstellungsprozess von Software ist die Eindeutigkeit der verwendeten Artefakte und des Ergebnisses besonders wichtig. Eine eindeutige Beschreibung des Zielsystems ist genauso wichtig, wie die eindeutige Beschreibung der Abhängigkeiten der Software.

Werden Teile der Software auf dedizierten, manuell erstellten und gewarteten Systemen entwickelt, können diese Systeme undokumentierte Abhängigkeiten bereitstellen. Werden diese Abhängigkeiten von der Software eingebunden, führt dies auf anderen Systemen ohne diese Abhängigkeit, zum Absturz. Ist die Abhängigkeit auf einem neuen Zielsystem vorhanden, allerdings in einer anderen Version, kann dies zu unerwartetem Verhalten oder ebenfalls zum Absturz führen.

An dieser Stelle bietet sich die Virtualisierung der verwendeten Systeme an. Dadurch kann sichergestellt werden, dass die Software immer unter den gleichen, definierten Bedingungen ausgeführt wird. Die vollständig automatisierte Erstellung von Virtualisierungsumgebung sichert die vollständige Beschreibung aller Abhängigkeiten. Nicht oder falsch Beschriebene Abhängigkeiten führen zu reproduzierbaren Fehlern.
Automatisch erstellt Systeme, die für die Virtualisierung verwendet werden, können archiviert und mit der Version der Software verknüpft werden. Dadurch steht eine besonders effiziente und nachvollziehbare Methodik zur Verfügung, um einen reproduzierbaren Stand der Software bereit stellen zu können. Dies unterstützt die Konsistenz, Transparenz, sowie Skalierbarkeit des Entwicklungsprozesses für die Software\footcite[vgl.][Kap. Why are containers important?]{learn-docker}.

\subsubsection{Varianten der Softwareauslieferung}

Bei der Auslieferung der Software können grob zwei Stufen der Automatisierung unterschieden werden. Eine Stufe ist die manuell angestoßene Auslieferung, bei der der Zeitpunkt der Auslieferung davon bestimmt wird, wann das automatisierte 
oder automatisiert-moderierte Skript ausgeführt wird. Diese Stufe wird auch als \glqq Continuous Delivery\grqq{} bezeichnet. Eine andere Stufe ist die vollautomatisierte Auslieferung. Dabei wird der
Zeitpunkt lediglich von der Durchlaufzeit einer Änderung durch die Qualitätskontrolle bestimmt. Da diese Auslieferung kontinuierlich alle Änderungen Ausliefert, wir diese Stufe auch als \glqq Continuous Deployment\grqq{} bezeichnet.
Des Weiteren werden unterschiedliche Auslieferungsstrategien (\glqq deployment strategies\grqq{}) unterschieden, es seien an
dieser Stelle nur exemplarisch das \glqq blue green deployment\grqq{} und das \glqq canary deployment\grqq{}\footcite[261-264 Deploying and Releasing Applications][]
{humble2010} genannt. Die Auslieferungsstrategien werden dabei je nach Anforderungen ausgewählt und haben Vorzüge wie 
Ausfallsicherheit, Bewertung der ausgelieferten Änderung oder Skalierbarkeit der Zielsysteme.

Die konkrete Auslieferung hat auf die Bereiche Continuous-Integration und Feature-Branches nur begrenzt Einfluss. Daher 
werden Continuous-Deployment-Strategien nicht weiter betrachtet.

\subsection{Validierung und Test}

Die Validierung einer Software ist ein sehr komplexes Unterfangen. Die vollständige Richtigkeit eines Programmes 
zu beweisen, scheitert bereits bei kleinen Softwareanwendungen, da die Programmkomplexität mit ihrem Umfang stark steigt. Zudem
können nur Teile validiert und getestet werden, welche im Vorfeld bestimmt worden sind. Unklare, mehrdeutige oder 
fehlende Anforderung machen einen Test unvollständig oder nehmen dem Test die Aussagekraft\footcite[S. 243 Kapitel 4.6][]{software-quality2008}.

Da eine Software nicht in verhältnismäßiger Zeit getestet werden kann, muss eine Strategie gewählt werden. Es muss bestimmt werden, welche Teile der Softwareanwendung getestet werden. Die Vollständigkeit der Tests, auch Testabdeckung genannt, wird gemessen. Die meisten Werkzeuge messen die Testabdeckung, durch die Nachverfolgung der von den Test durchlaufenen Teile der Softwareanwendung. Bei der Entscheidung, welche Strategie gewählt wird, fallen verschiedene Kriterien ins Gewicht. Zum einen muss betrachtet werden, welche Anforderungen mit welchem Testverfahren getestet werden können. Zum anderen, welche Granularität für jeden Test verwendet wird.
Je nachdem, welche Strategie gewählt wird, hat dies Auswirkungen auf die Ausführungszeit der Tests. Diese Zeit wirkt sich direkt auf die Produktivität des Entwicklungsteams aus. Ohne die Testresultate kann keine Aussage über die Funktionsfähigkeit der Softwareanwendung getroffen werden. Damit ist die Freigabe einer Softwareanwendung oder eines Inkrementes der Softwareanwendung direkt von der Durchlaufgeschwindigkeit der Tests abhängig. 

Ein wichtiger Punkt ist hier auch die aktive Wartung der Testsammlung(Test-Suite). Jeder Fehler, der nicht von der Testsammlung erfasst wurde, muss spätestens nach der Beseitigung des Fehlers auch durch die Testsammlung validiert werden. Diese Regressions-Sicherheit stellt in vielen Testsammlungen häufig den größten Teil der Tests\footcite[vgl.][]{software-quality2008}. Ein direkter Nachteil einer großen Test-Suite ist der Aufwand für ihre Pflege. Testabdeckung, Wartbarkeit der Tests und Aussagekraft der Tests sollte daher in der Teststrategie gegeneinander abgewogen werden.

\subsubsection{Software-Metriken}
\label{subsubsec:base-metrics}
Software-Metriken können systematisch und quantitativ Aspekte des Software-Codes erfassen. Als Teilaspekt der Code-Analyse können Software-Metriken dazu verwendet werden, Kenngrößen des Software-Systems zu quantifizieren und damit Bewertung und Vergleich mit anderen Code-Ständen ermöglichen.\footcite[S.247][]{software-quality2008}

Eine nützliche Software-Metrik sollte mehrere Gütekriterien erfüllen. Die Metrik sollte objektiv sein, was bei einer maschinellen Erhebung und Verarbeitung weitestgehend gegeben ist. Die Ergebnisse einer Metrik sollten wiederholbar und damit robust sein. Zudem sollten die Ergebnisse vergleichbar sein, um verschieden Softwareversionen gegeneinander bewerten zu können. Eine Metrik sollte mit dem untersuchten Merkmal korrelieren. Die Korrelation ist immer dann besonders hoch, wenn der mit der Metrik erhobene Wert sich stark ändert, wenn auch das betrachtet Merkmal sich ändert. Abschließend sollte die Erhebung der Metrik in Relation zu der Güte ihrer Aussage stehen. Metriken mit einem hohen Erhebungsaufwand, aber ohne Relevanz für den Entscheidungsprozess, sind nicht wertvoll.\footcite[S.248 ff][]{software-quality2008}

Da Tests und Software-Metriken gut anwendbare Möglichkeiten sind, um automatisiert Einschätzungen der Software vorzunehmen, werden diese Bereiche erneut im Kapitel~\ref{subsec:main-metrics} aufgegriffen und näher auf deren Softwareanwendung eingegangen.

Im Folgenden werden kurz bekanntere Metriken vorgestellt.\footcite[S.249 ff][]{software-quality2008}

\paragraph{LOC und NCSS} sind Kenngrößen für den Umfang des Sofware-Codes. \glqq Lines of Code\grqq{} (LOC) geben die Anzahl von Code-Zeilen an. \glqq Non Commented Source Statements\grqq{} (NCSS) ist die gleiche Kenngröße, allerdings abzüglich der Kommentarzeilen. Beide Metriken haben nur ein sehr schwache Aussage über den Umfang des Software-Codes, da die Anzahl an Zeilen mit keiner Qualitätsgröße sinnvoll korreliert.

\paragraph{Halstead-Metrik} bezeichnet eine Kenngröße für die Programmkomplexität. Die Ermittlung erfolgt durch die Analyse der lexikalischen Struktur vom Software-Code. Dabei wird der Code in Operatoren und Operanden eingeteilt und deren Auftrittscharakteristik analysiert. Kritik an dieser Metrik entsteht durch die offene Definition von Operanden und Operationen. Als Folge sind verschiedene Programmiersprachen nur bedingt Vergleichbar durch diese Metrik. Des Weiteren wird in Frage gestellt, ob alle Kerngrößen der Metrik überhaupt mit der lexikalischen Analyse ermittelt werden können.

\paragraph{McCabe-Metrik} bezeichnet eine weitere Kennzahl für die Programmkomplexität. Zentrale Größe der Metrik ist die \glqq zyklomatische Komplexität\grqq{} als Maß der Anzahl von Kanten und Knoten eines Kontrollflussgraphen der Softwareanwendung. Somit korreliert die zyklomatische Komplexität mit den möglichen Anwendungsdurchläufen und -zuständen.

\paragraph{Komponenten-Metriken} bezeichnen eine Sammlung von Umfangs- und Komplexitätsmetriken für objektorientierten Software. Eine Aussage über den Umfang von Klassen wird unter anderem durch die Anzahl von Attributen und Methoden getätigt. Die Komplexität wird durch die Tiefe und Häufigkeit von objektorientierten Techniken gemessen. Diese Techniken sind unter anderem Vererbung, Überladung und Überschreiben von Methoden und Attributen. Komponenten-Komplexitätsmetriken liefern zudem eine Aussage zum Kopplungsgrad der Klassen untereinander.

\paragraph{Struktur-Metriken}\label{par:structure-metrics}, insbesondere mit den Kopplungsmetriken, als wichtigste Untergruppe, beschreiben ebenfalls die Programmkomplexität. Besonders wichtig sind die Kenngrößen Fan-In und Fan-Out. Fan-Out ist die Anzahl an Verwendungen einer Klasse in anderen Klassen. Fan-In die Anzahl der verwendeten anderen Klassen in einer Klasse. 

\subsubsection{Software-Verifikation}
\label{subsubsec:base-verification}
Verfahren der Software-Verifikation sind die Deduktion, Modellprüfung und der abstrakten Interpretation\footcite[S. 333 ff][]{software-quality2008}. Welches Verfahren angewendet wird, ist abhängig von einer Kombination aus Aussagekraft und dem Aufwand, der dafür aufgewendet werden kann. Ein aufwändiges, manuelles Verfahren skaliert vergleichsweise schlecht, wie in Abbildung~\ref{software-quali-verification} dargestellt.

\begin{figure}[htbp]
  \includegraphics[
    width=\textwidth,
    height=\textheight,
    keepaspectratio
  ]{resources/software-quali-verification.pdf}
  \caption{Verhältnis von Ausdrucksstärke und Skalierbarkeit der Verifikationstechniken\protect\footnotemark}
  \label{software-quali-verification}
\end{figure}
\footcitetext[vgl.][S. 337]{software-quality2008}

\paragraph{Deduktion} ist stark aus der klassischen mathematischen Beweisführung entlehnt. Dabei werden Vor- und Nachbedingung durch Umformungen sukzessive ineinander überführt. Für vollständige Softwareanwendungen ist dieses Verfahren, ein großteils manueller Vorgang. Für Teile der Software wird die Deduktion auch automatisiert verwendet. Einfache Kontrollflussanalysen erlauben automatisierte Beurteilungen von Code-Abschnitten. Nicht erreichbare Code-Abschnitte, unvollständige Ausdrücke oder Typ-Fehler in Methodenaufrufen, können gut erkannte werden. In vielen Entwicklungsumgebungen ist diese Form der Deduktion bereits integriert\footcite[vgl.][S. 2]{automated-deduction}. Unter anderem sind dadurch Kontrollflussanalysen möglich. Unerreichbare oder nicht vollständige Ausdrücke, können so aufgezeigt werden.

\paragraph{Modellprüfung} überführt ein Programm in eine Kripke-Struktur, ähnlich einem endlichen Automaten und beweist oder widerlegt diese Struktur mithilfe von Traversierungsalgorithmen. Die algorithmische Komplexität dieses Verfahrens sorgt für eine weitestgehend ausschließliche Softwareanwendung für sicherheitskritische Software-Systeme.

\paragraph{Abstrakte Interpretation} ist eine semi-formale Verifikationstechnik. Sie kombiniert statische Programmanalyse und Generalisierung von Datenbereichen, um verschieden dynamische Eigenschaften eines Programmes zu verifizieren.

\vspace{1em}

Für die vollautomatische Betrachtung von Software-Änderungen sind daher die abstrakte Interpretation und die bereits beschriebenen Software-Tests besser geeignet. 

\section{Continuous-Integration}

Continuous-Integration wurde sehr treffend beschrieben \footcite[vgl.][]{fowler2006}

\blockquote {Continuous-Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible.}

Continuous-Integration wurde erstmals von Kent Beck als Teil einer Reihe von Praktiken des Entwicklungsvorgehens \glqq Extreme Programming\grqq{}\footcite[vgl.][]{kent1999} beschrieben. Dabei sollte die Integration der einzelnen Entwickler-Codeständen gefördert werden. Entwickler sollen ihre Änderungen täglich oder häufiger abgleichen und durch einen automatisierten Vorgang als lauffähig beweisen. Damit konnten Fehler nahe des Zeitpunkt des Auftretens gefunden werden. Fehler, die erst mit der Integration von Änderungen anderer Entwickler auftreten, können so gefunden werden. Damit fordert Continuous-Integration eine aussagekräftige Testabdeckung mit automatischen Tests. Des Weiteren können und sollten Entwickler einen entdeckten Fehler direkt beheben, da sonst alle anderen Teammitglieder behindert werden.

\subsection{Vorgehensweise mit Continuous-Integration}

Als erstes wird einen Basis zur Integration der gemeinsamen Arbeit genutzt. Ein Versionsverwaltungssystem bildet diese Grundlage. Dadurch werden nachvollziehbar, zeitlich sortiert und wiederherstellbar alle Änderungen gesammelt. Alle Änderungen sollen, auf einem gemeinsamen Stand, regelmäßig zusammengeführt werden. Die Arbeitsweise, alle Änderungen auf einem Zweig zu sammeln und kontinuierlich zu integrieren, wird auch Trunk-Based-
Development genannt. Abgeleitet von der Bezeichnung des häufig genutzt Versionsverwaltungssystems Subversion. Der Hauptzweig in Subversion wird im Allgemeinen als \glqq Trunk\grqq{} bezeichnet. Während Continuous-Integration und Trunk-Based-Development als getrennte Methodiken geführt werden\footcite[vgl.][]{trunkbaseddevelopment}, treten beide Begriffe häufig in Kombination oder synonym auf\footcite[vgl.][]{fowler-feature-branch}.

Neben einem Integrationssystem ist auch eine einheitliche Form der Erstellung der Software notwendig. Ein einheitliches Verständnis der Codebasis, verlangt auch ein einheitliches Verständnis der Erstellung der Software. Die Software sollte daher, wie in Kapitel~\ref{sec:automation-software}~\nameref{sec:automation-software} beschrieben, erstellt werden.


\subsection{Vorteile von Continuous-Integration}

Wird der Codestand aller Entwickler häufig integriert und nach jeder Änderung durch einen automatisierten Build verifiziert, erhalten alle Beteiligten regelmäßig Rückmeldung zum gemeinsamen Codestand.
Die häufige Integration von Quellcode fördert zudem die Verwendung von modernen Entwicklungspraktiken. Modularisierung, Konfigurationsmanagement, automatische Erstellung und Wiederverwendbarkeit von Artefakten, sowie das Achten auf Testbarkeit der Softwareanwendung sind wichtig. Die vollständige Erfüllung dieser Kriterien fordert und fördert eine gute Entwicklungskultur.

\subsection{Nachteile von Continuous-Integration}

Während die Vorteile von Continuous-Integration deutlich sind, gibt es auch Kosten, die mit dieser Technik einhergehen. Die komplette Automatisierung erfordert einen erheblichen Aufwand in der initialen Einrichtung und einen häufig unterschätzten Aufwand in der Aufrechterhaltung. 
Gerade weniger wichtige Projekten oder Projekte unter besonders hohem zeitlichen Druck, neigen dazu wichtige Bestandteile von Continuous-Integration zu vernachlässigen. Mögliche Symptome sind das Sinken der Testabdeckung. Oder Teile des Konfigurationsmanagements werden umgangen und essentielle Abhängigkeiten von Hand gepflegt. Dieses Verhalten widerspricht offensichtlich der langfristigen Wertschöpfung und bricht mit den Prinzipien von Continuous-Integration. Menschliche Schwächen sind auch in Continuous-Integration noch ein Problem. Mit regelmäßigem 
Training, Erfahrung und Selbstdisziplin kann dem entgegen gewirkt werden. In Kapitel~\ref{sec:human-fail} wird das Thema Entwicklerdisziplin erneut aufgegriffen.

\section{Virtualisierung}

Im Kapitel ~\ref{sec:automation-software}~\nameref{sec:automation-software} wurden bereits Stärken der Virtualisierung genannt. Gerade bei der vollautomatisierten Bereitstellung von Systemen, können Stärken der Virtualisierung genutzt werden.

Virtualisierung bietet eine Abstraktionsschicht für verschiedene Komponenten. Dies ermöglicht Abhängigkeiten zu Hardware, Software und deren Konfiguration transparent zu gestalten. Der unter Programmierern bekannte Spruch, \glqq It works on my machine\grqq{}, verdeutlicht das Problem. Häufig benötigen die Komponenten einer Softwareanwendung weitere Abhängigkeiten. Abhängigkeiten zum Betriebssystem, zu Software von Drittanbietern oder zu Hardwarekomponenten sind Bestandteil
vieler Softwareanwendungen. Im Verlauf der Softwareentwicklung ist es daher wichtig, diese Abhängigkeiten zu beschreiben. Diese Beschreibung ist eine wertvolle Basis, um ein System inklusive aller Abhängigkeiten zur Verfügung zu stellen.

Die manuelle Bereitstellung eines Systems ist aufwändig und verzögert damit zusammenhängende Entwicklungsabläufe. Eine automatisierte Erstellung und Bereitstellung der benötigten Systeme ist daher empfehlenswert. Um unerwartetes Verhalten 
der auf den Systemen ausgeführten Softwareanwendungen zu vermeiden, sollten die Systeme vollständig beschrieben sein. Die automatische Generierung der virtuellen Umgebung und die Ablage in gepackter Form in einem Archiv bietet weitere Vorteile. Der Zugriff auf die Virtualisierung und damit die Bereitstellung der Systeme wird dadurch besonders transparent 
und wenig aufwändig.

Durch die Virtualisierung kann ein System reproduzierbar und schnell in einem vorher definierten Zustand bereit gestellt werden. Dies verkürzt Arbeitsabläufe für den Test einer Softwareanwendung. Zum einen können schnell Änderungen an der Softwareanwendung getestet werden, zum anderen kann die Anwendung in zahlreichen Konfigurations- und Abhängigkeitskombinationen getestet werden.

\subsection{Virtualisierung mit Virtual-Machines}

Virtualisierung mit Virutal-Machines abstrahiert eine vollständige Systemumgebung. Diese ermöglicht unter anderem ein Betriebssystem innerhalb eines anderen Betriebssystems zu betreiben. Da eine vollständige Systemumgebung mit Hardwarekomponenten emuliert wird, lassen sich Softwareanwendungen auf verschiedenen Virtual-Machines komplett unabhängig voneinander betreiben.
Durch den höhren Abstraktionsgrad der Virtualisierung, ist der Systemressourcenverbrauch signifikant höher, als beim Betrieb einer Softwareanwendung in einem nicht virtualisierten System. Virtuelle Maschinen skalieren somit gut in Erstellung und Verwaltung. Im Gegensatz dazu skaliert der Systemressourcenverbrauch deutlich schlechter. Durch den geringeren Verwaltungsaufwand können dennoch große Cluster kosteneffizient betrieben werden, wie viele Cloud-Services zeigen\footcite[vgl.][]{a-cloud-guru-cost}.

\subsection{Containerverwaltung mit Docker}

Docker ist keine Virtualisierung-Software im eigentlichen Sinne. Ein Docker-Container ist eine standardisierte Umgebung, in der ein Softwareanwendung laufen kann. Ein Docker-Server verwaltet die Docker-Container und regelt deren Sicherheits- und Leistungsbedürfnisse. Docker läuft somit als Anwendung auf einem System und ist an dessen Spezifikationen für Betriebssysteme und Hardwarekomponenten gebunden\footcite[vgl.][Kap. Why are containers important]{learn-docker}.

Softwareanwendungen werden in Form von Docker-Images abgespeichert und können in unbegrenzt vielen Docker-Container ausgeführt werden. Änderungen im Inhalt des Docker-Containers sind auf diesen beschränkt und haben keinen Einfluss auf das Docker-Image aus dem er erstellt wurde.

Als größter Vorteil von Docker wird die Standardisierung der Softwareanwendung bewertet. Durch die Einheitliche Behandlungen der Software, lassen sich Konfiguration, Auslieferung und Aktualisierung deutlich vereinfachen und beschleunigen. Des Weiteren sind die Container voneinander getrennt und unterstützen somit Sicherheitsaspekte. Sowohl die Datenintegrität, als auch Softwarekompatibilität sind deutlich einfacher zu handhaben\footcite[vgl.][]{ten-benefits-docker}.

Mit Docker und virtuellen Maschinen wurden zwei Möglichkeiten geschaffen, Software weiter zu vereinheitlichen. Standardisierung ist eine wesentliche Basis für Modularisierung und Automatisierung.

\section{Dezentrale Versionierung mit Git}
\label{distributed-vcs-git}

Im Kapitel~\ref{subsec:konfigurationsverwaltung} Konfigurationsverwaltung wurde bereits auf das Thema Versionsverwaltung 
eingegangen. Als das populärste System im Open-Source-Bereiche\footcite[vgl.][]{openhub-pie-chart} und eines der großen Systeme 
im kommerziellen Bereich\footcite[vgl.][]{g2crowd2018}, wird nun anhand von Git erläutert, wie die dezentrale Versionsverwaltung 
grundlegend verwendet wird. Außerdem kommen einige git-spezifische Merkmale hinzu, welche bei der visuellen Aufbereitung 
wichtig werden.

Git wurde vom Linux-Schöpfer Linus Torvalds im April 2005 initiiert, aus der Not heraus sich vom vorher verwendeten \glqq BitKeeper\grqq{}-System zu lösen, da diese nicht mehr mit der eigenen Open-Source-Lizenz in Einklang zubringen war. Git wurde von vornherein als verteiltes, vor Verfälschungen sicheres und effizientes Versionsverwaltungssystem entworfen. 
Diese Eigenschaften und die Software-Plattform \glqq GitHub\grqq{}, machten Git in den letzten 13 Jahren zum de-facto Standard für Versionsverwaltung\footcite[vgl.][]{heise-torvald-git2015}.

Als dezentrales Versionsverwaltungssystem ist einer der großen Vorteile von Git, dass die vollständige Historie der Dateien lokal verfügbar ist. Während zentrale Versionsverwaltungssysteme immer einen dedizierten Server benötigen, können dezentrale Varianten über Peer-To-Peer-Schnittstellen kommunizieren. Diese Eigenschaft sorgt neben einer hohen Ausfallsicherheit und Datenredundanz, auch für eine hohe Flexibilität und die Möglichkeit, zahlreiche Arbeitsweisen von Softwareentwicklern zu unterstützen.

\subsection{Definition und Verwendung der Git-Artefakte}

Die Verwendung von git gestaltet sich vergleichsweise einfach. In wenigen Schritten kann Git installiert \footcite[vgl.][]{git-scm-install} und mit der Verwendung begonnen werden. 
Git nutzt bestimmte Artefakte, um seine Aufgaben zu erfüllen. Darunter Repositories, Commits, Branches und Tags. Zudem 
werden Mechanismen bereitgestellt, um mit diesen Artefakten zu interagieren. Neben zu erwartenden Mechanismen wie 
Hinzufügen und Löschen, werden auch Speichern- und Ladeaktionen (\glqq push\grqq{} und \glqq pull\grqq{}) angeboten\footcite[vgl.][]{git-essentials-2017}.

\begin{figure}[htbp]
  \includegraphics[
    width=\textwidth,
    height=\textheight,
    keepaspectratio
  ]{resources/git-workflow.pdf}
  \caption{Git-Workflow}
  \label{git-workflow}
\end{figure}
Die Grafik~\ref{git-workflow} stellt das Zusammenspiel der Artefakte und den Standard-Git-Worflow\footcite[vgl.][]{osteele-git-workflow} dar.

\paragraph{Repositories} sind die größte Verwaltungseinheit in Git. Diese enthalten alle Git-Objekte, wie Referenzen, 
Commits, Trees, Blobs, sowie die lokale Konfiguration. Repositories können andere Repositories referenzieren oder 
referenziert werden, dabei können viele gängige Protokolle verwendet werden (http, ssh, ftp, absolute Pfade)

\paragraph{Commits} sind zeitlich determinierte, persönliche und kommentierte Referenzen auf einen konkreten Arbeitsstand 
(\glqq Snapshot\grqq{}) des Repositories. Ein Commit kann auf mehrere Eltern-Commits verweisen und kann von beliebig vielen
anderen Commits referenziert werden. Bei Betrachtung aller Commits, bilden dieser daher einen gerichteten Graphen. 
Mehrere Eltern-Commits entstehen bei der Zusammenführung von Branches.

\paragraph{Branches} sind, im Gegensatz zu vielen anderen Versionverwaltungssystemen, in Git lediglich Verweise auf einen 
bestimmten Commit. Wenn ein Branch aktiv ist, dann wird mit jedem Commit auf diesem Branch, der Verweis auf den neuen 
Commit aktualisiert.

\paragraph{Tags} sind genauso wie Branches einfache Verweise auf einen Commit, allerdings verändert sich die Position 
eines Tags nach einem Commit nicht.

\paragraph{Remotes} sind Verweise in der Konfiguration eines Repositories auf andere Repositories. Dies wird im 
allgemeinen genutzt, um Änderungen von dort zu holen oder dorthin zu bewegen. Es können beliebig viele Remotes für jedes 
Repository definiert werden. Zudem kann beliebig definiert werden, welche Branches von einem Repository Änderungen 
erhalten oder dieses aktualisieren.

\paragraph{Blobs} sind die komprimierten Inhalte einer Datei.

\paragraph{Trees} sind Referenzen von Blobs, und stellen damit im einfachsten Fall eine Ansicht eines Verzeichnisses und 
seiner Dateien dar.

\paragraph{Hashes} sind die Ergebnisse einer Hash-Funktion. Hash-Funktionen bilden eine eindeutige Abbildung einer Eingangsmenge auf eine Ausgabemenge. Dadurch ist es möglich auch komplexere Eingabewerte auf einen kürzen Eingabewert abzubilden. Unter anderem werden Hash-Funktion für Hash-Tabellen als Index verwendet. 

In Git werden Hashes als allgemeine Referenzierungsmöglichkeit verwendet. Alle Relationen werden darüber beschrieben und sind daher über alle Repositories gleich. Git verwendet für die Bildung der Hashes den SHA1-Alogrithmus. 

Die Forderung statt des SHA1- den SHA256-Algorithmus zu verwenden, wurde abgelehnt. Linus Torvalds schätzt die Sicherheitsbedenken als nicht zutreffend ein\footcite[vgl.][]{git-sha-torvalds}. Diese 
Aussage wurde auch von GitHub bestätigt\footcite[vgl.][]{git-sha-github}.

\subsection{Interne Arbeitsweise}

Git verwendet intern einen Mix aus Referenzen, Indexierung, Komprimierung und Hashing. Zudem werden keine 
Differenzmengen, wie etwa in Subversion abgelegt, sondern immer vollständige Dateiinhalte. Für manche Projekte, wie das 
Mozilla-Projekt\footcite[vgl.][]{kernel-git-svn} kann dies eine Speicherplatzreduktion erreichen. Im Allgemeinen kann dies aber nicht bestätigt werden\footcite[vgl.][]{svn-vs-git}. Durch die vollständige Speicherung der Inhalte in jedem vernetzten Git-Repository, ist der Speicherbedarf insgesamt höher. Insbesondere für große Dateien, mit nur geringfügigen Änderungen ergeben sich deutliche Unterschiede zu Versionssystemen mit Differenzmengen. Für schlecht komprimierbare Dateien oder Dateien mit schlechten Differenzmengen, wie etwa Binärdateien, sollte allerdings grundsätzlich ein Artefakt-
Repository in Betracht gezogen werden. 

Seit wenigen Jahre wird \glqq Git Large File Storage\grqq{} (Git LFS)\footcite{git-lfs} angeboten. Dadurch werden ausgewählte Dateien nur als Referenz in Git gesichert. Dies schwächt den früheren Nachteil deutlich ab.

Um die interne Arbeitsweise von Git zu erläutern, müssen die Zusammenhänge von Commits, Trees, Blobs und Hashes 
verdeutlicht werden.

Die Git-Artefakte werden von Git immer anhand ihres Hashes abgelegt. Dabei wird ein 40-stelliger SHA1-Hash verwendet. Die 
notwendigen Stellen zu Referenzierung sind aber häufig deutlich geringer, so können im allgemeinen Gebrauch deutlich 
verkürzte Zeichenketten verwendet werden. Im allgemeinen etwa 5 Stellen oder bei großen Projekten, wie dem Linux-Kernel, 
12 Stellen.

Der Hash von Blobs ist eine Abbildung ihres Inhaltes. Daher werden Dateien mit dem gleichen Inhalt, auch immer auf den 
gleichen Blob abgebildet, unabhängig vom Verzeichnis in dem sie sich befinden oder wie oft sie geändert wurden. Da ein 
Blog nur die Abbildung des Inhaltes der Datei ist, führen Merkmale wie Zeitstempel oder Dateirechte zu keiner Änderung 
des Hashes.

\begin{figure}[htbp]
  \includegraphics[
    width=\textwidth,
    height=\textheight,
    keepaspectratio
  ]{resources/git-internal-work.pdf}
  \caption{Verweise der Artefakte von Git}
  \label{git-internal-work}
\end{figure}

Wie auch beim Blob sind Trees nur eine Abbildung ihres Inhaltes. Da Trees aber als eine Art Verzeichnis zu verstehen 
sind, referenzieren Trees andere Trees und Blobs. Damit würde eine Folge von Commits, die zuerst eine Datei erzeugt und 
schließlich wieder entfernt, am Ende wieder auf den gleichen Tree verweisen.

Commits schließlich verweisen auf einen oder mehrere Vorgänger-Commits, einen Tree, einen Autor und einen Commiter. Hat ein Commit mehr als einen Vorgänger, wird im Allgemeinen von einem Merge-Commit gesprochen.

Dieser einfache, gut skalierbare Aufbau ermöglicht eine sehr leichtgewichtige Erstellung von Branches und damit 
zahlreiche flexible Arbeitsweisen.

\subsection{GitHub-Workflow}

GitHub ist heute die größte Plattformen für quelloffene Softwareprojekte\footcite[vgl.][]{github-marketshare-datanyze}. Durch die 
rasante Verbreitung von Git, wurde GitHub bei Open-Source-Projekten schnell zur Alternative für Sourceforge
\footcite[vgl.][]{heise-github-2011}. Damit einhergehend hatte der GitHub-Workflow\footcite[vgl.][]{github-workflow-intro} auf die Git-
Gemeinschaft hohen Einfluss.

Der GitHub-Workflow ist eine einfache Arbeitsweise, die auf Branches und manuellen Begutachtungen(Reviews) basiert. 
Änderungen gelangen nur zurück auf den Hauptstrang, wenn sie zuvor einem Review unterzogen worden. Dieser Schritt ist ein 
entscheidender Unterschied zum Trunk-Based-Workflow der mit Continuous-Integration propagiert wird, da hier der Code-
Review erst nach der Integration mit dem Hauptzweig durchgeführt werden kann.

\subsection{Gitflow}
\label{subsec:gitflow}

Gitflow ist ein weiterer bekannter Workflow mit Git\footcite[vgl.][]{nvie-git-branch-model}. Im Gegensatz zum simplen GitHub-
Workflow liegt beim Gitflow der Schwerpunkt auf der Erstellung eines Releases, also einer Menge an Commits, häufig auch
von verschiedenen Autoren.

In Anlehnung an viele bekannte Modelle unterscheidet Gitflow zwischen Produktivzweig(master), außerplanmäßiger 
Anpassung(hotfix), Auslieferungszweig(release), Entwicklungszweig(development) und zahlreichen Feature- und Bug-Zweigen.

Gitflow unterscheidet wie viele andere Git-Arbeitsmodelle zwischen langlebigen und kurzlebigen Branches. Die langlebigen 
Branches sind hierbei \glqq master\grqq{} und \glqq development\grqq{}. Alle anderen Branches sind kurzlebige Branches(\glqq supporting
branches\grqq{}), die immer erst erstellt werden, wenn sie notwendig sind und gelöscht werden, sobald sie ihren Zweck erfüllt
haben.

\begin{figure}[htbp]
  \includegraphics[
    width=\textwidth,
    height=\textheight,
    keepaspectratio
  ]{resources/git-flow.pdf}
  \caption{Gitflow-Workflow\protect\footnotemark}
  \label{git-flow}
\end{figure}
\footcitetext[vgl.][]{nvie-git-branch-model}

Der konkrete Arbeitsfluss wird in der Abbildung~\ref{git-flow} zusammengefasst. Es wird deutlich, dass nur Commits von 
Release- und Hotfix-Branches auf den Master-Branch gelangen können. Desweiteren ist herauszustellen, dass nach der 
Erstellung des Release-Branches keine Änderungen mehr vom Development-Branch zugeführt werden. Diese Trennung der 
Änderungsflüsse führt dazu, dass bereits an neuen Themen gearbeitet werden kann, ohne dass der Release behindert wird. 
Des Weiteren ist klar geregelt, dass alle Features immer erst im Development-Branch integriert werden müssen.

\section{Feature-Branches}
\label{sec:feature-branches}
Die Idee hinter einem Feature Branch ist, dass für jedes neue Merkmal, beziehungsweise für jede neue Anforderung ein 
neuer Branch in der Versionsverwaltung erstellt wird. Ziel von Feature-Branches ist die Isolierung von Änderungen, um die 
anderen Zweige nicht zu blockieren und Änderungen erst nach Test und Abnahme zusammenzuführen. Dabei sollen mehrere 
Änderungen parallel entwickelt werden können, ohne dabei zu einem bestimmten Zeitpunkt alle Änderungen in einen 
Hauptzweig übernehmen zu müssen.

Prinzipiell ist diese Technik unabhängig von der verwendeten Versionsverwaltung, erhielt aber vor allem durch dezentrale 
Versionsverwaltungssysteme an Bedeutung. Der Grund dafür liegt in der deutlich einfacheren und weniger aufwändigen 
Erstellung von Branches und deren Zusammenführung.

Die Methode Feature-Branches wird kontrovers diskutiert\footcite[vgl.][]{fb-revisited}. 
Dabei wird angemahnt, dass die Abspaltung in einen Branch, das Problem der Zusammenführung von Codeänderungen nicht behebt, sondern verschlimmert\footcite[vgl.][]{fowler-feature-branch}. So wird der Aufwand, der benötigt wird um zwei Verzweigungen zusammenzuführen, potentiell immer höher, um so mehr Änderungen hinzukommen. Die Aufwandserhöhung kann soweit gehen, dass ein psychologischer Faktor 
hinzukommt, der die Entscheidung zur Zusammenführung weiter belastet. Diese Zusammenführung wird dann auch als \glqq big scary merge\grqq{} oder \glqq big bang merge\grqq{} bezeichnet.

Kontrovers dazu wird hervorgehoben, dass Feature-Branches Blockaden im Arbeitsfluss vermeiden und durch Entwicklerdisziplin und Automatisierung, die Angst vor dem \glqq big scary merge\grqq{} mildern\footcite[vgl.][]{ci-is-dead}. Dazu wird verlangt, Feature-Branches nur für kleine Änderungen zu verwenden und durch Hilfe von Automatisierung, schnelle Rückmeldung über die Codequalität zu erhalten. Des Weiteren werden durch moderne \glqq pull based\grqq{}\footcite[vgl.][]{github-about-pull} Verzweigungsstrategien, Entwickler dazu angehalten Codeprüfungen durch andere Entwickler einzufordern. Diese Lösungsstrategien werden in Kapitel~\ref{ch:visu_meth} weiterführend betrachtet.